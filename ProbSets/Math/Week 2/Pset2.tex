	 \documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{setspace}
\pagestyle{fancy}
\usepackage{amsmath, amsthm, amssymb, amsfonts, mathtools, xfrac,mathrsfs}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx,dsfont}
\usepackage{braket, bm}

\everymath{\displaystyle}
\headheight=20pt


\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var} {\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\F}{\mathbb{F}}

\DeclareMathOperator{\Tr}{Tr}

\def\mean#1{\left< #1 \right>}
 
\newcommand*\rfrac[2]{{}^{#1}\!/_{#2}}
 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2}]}{\end{trivlist}}

\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2}]}{\end{trivlist}}

\title{Homework}
\lhead{Math OSM Lab}
\chead{Homework 2 - 7/3/17}
\rhead{Dan Ehrlich}
 
\begin{document}

\begin{problem}{1.} We have that $\| x + y \|^2 = \|x\|^2 + \|y\|^2 + 2\| x\|\|y\|\cos(\theta)$ and $\| x - y \|^2 = \|x\|^2 + \|y\|^2 - 2\| x\|\|y\|\cos(\theta)$ where $\cos(\theta) = \frac{\langle x,y \rangle}{\| x\|\|y\|}$ which is proven in the textbook. 
\begin{itemize}
\item [(i)]  $\frac{1}{4}(\| x + y \|^2 - \| x - y \|^2 )= \frac{1}{4} (\|x\|^2 + \|y\|^2 + 2\| x\|\|y\|\cos(\theta) - \|x\|^2 - \|y\|^2 + 2\| x\|\|y\|\cos(\theta)) = \frac{1}{4} (4\| x\|\|y\|\cos(\theta)) = \| x\|\|y\| \frac{\langle x,y \rangle}{\| x\|\|y\|} = \langle x,y \rangle$
\item [(ii)]$ \frac{1}{2}(\| x + y \|^2 + \| x - y \|^2) = \frac{1}{2}( \|x\|^2 + \|y\|^2 + 2\| x\|\|y\|\cos(\theta) + \|x\|^2 + \|y\|^2 - 2\| x\|\|y\|\cos(\theta)) =  \frac{1}{2}(2(\|x\|^2 + \|y\|^2)) = \|x\|^2 + \|y\|^2$
\end{itemize}
\end{problem}

\begin{problem}{2.} 
\begin{equation*}
\begin{aligned}
& \frac{1}{4}(\| x + y \|^2 - \| x - y \|^2 + i\| x - iy \|^2 - i\| x + iy \|^2) = \langle x,y \rangle + \frac{1}{4}( i\| x - iy \|^2 - i\| x + iy \|^2) = \\
& = \langle x,y \rangle + \frac{1}{4}( -i( ||x||^2 +\langle x, iy \rangle +\langle  iy ,x\rangle +||y||^2  - ||x||^2 +\langle x, iy \rangle +\langle  iy ,x\rangle -||y||^2)) = \\
& = \langle x,y \rangle  + \frac{i}{4}(2i\langle x, y \rangle  - 2i\langle y,x \rangle ) = \langle x,y \rangle
\end{aligned}
\end{equation*}
\end{problem}

\begin{problem}{3.} \hfill
\begin{itemize}
\item [(i)] Let $f(x) = x$ and $g(x) = x^5$. Then 
$$\theta = \cos^{-1} \big(\frac{\langle f,g \rangle}{\| f\|\|g\|} \big )= \cos^{-1}\big( \frac{\int_0^1 x^6 dx}{ \sqrt{\int_0^1 x^2 dx}\sqrt{\int_0^1 x^{10}dx}}\big) =\cos^{-1} \big(\frac{1/7}{\sqrt{1/3}\sqrt{1/11}}\big) $$
\item [(ii)] Let $f(x) = x^2$ and $g(x) = x^4$. Then 
$$\theta = \cos^{-1}\big( \frac{\int_0^1 x^6 dx}{ \sqrt{\int_0^1 x^4 dx}\sqrt{\int_0^1 x^{8}dx}}\big) =\cos^{-1} \big(\frac{1/7}{\sqrt{1/5}\sqrt{1/9}}\big)$$
\end{itemize}
\end{problem}

\begin{problem}{8.}  \hfill
\begin{itemize}
\item [(i)] 
$$\langle \cos(t), \sin(t) \rangle  = \frac{1}{\pi} \int_{-\pi}^{\pi} \cos(t) \sin(t) dt = \frac{1}{\pi} * 0 = 0 $$
$$\langle \cos(t), \cos(2t) \rangle = \frac{1}{\pi} \int_{-\pi}^{\pi} \cos(t) \cos(2t) dt = 0 $$
$$\langle \cos(t), \sin(2t) \rangle = \frac{1}{\pi} \int_{-\pi}^{\pi} \cos(t) \sin(2t) dt = 0 $$
$$\langle \cos(t), \cos(t) \rangle = \frac{1}{\pi} \int_{-\pi}^{\pi} \cos(t) \cos(t) dt = 1 $$
$$\langle \sin(t), \cos(2t) \rangle = \frac{1}{\pi} \int_{-\pi}^{\pi} \sin(t) \cos(2t) dt = 0 $$
$$\langle \sin(t), \sin(2t) \rangle = \frac{1}{\pi} \int_{-\pi}^{\pi} \sin(t) \sin(2t) dt = 0 $$
$$\langle \sin(t), \sin(t) \rangle = \frac{1}{\pi} \int_{-\pi}^{\pi} \sin(t) \sin(t) dt = 1 $$
$$\langle \cos(2t), \sin(2t) \rangle = \frac{1}{\pi} \int_{-\pi}^{\pi} \cos(2t) \sin(2t)dt = 0 $$
$$\langle \cos(2t), \cos(2t) \rangle = \frac{1}{\pi} \int_{-\pi}^{\pi} \cos(2t) \cos(2t) dt = 1 $$
$$\langle \sin(2t), \sin(2t) \rangle = \frac{1}{\pi} \int_{-\pi}^{\pi} \sin(2t) \sin(2t) dt = 1$$
\item [(ii)] $$\|t\| = \sqrt{\frac{1}{\pi} \int_{-\pi}^{\pi} t^2 dt}  = \sqrt{\frac{2\pi^2}{3}}$$
\item [(iii)] 
\begin{equation*}
\begin{aligned}
&\textit{proj}_X(\cos(3t)) = \langle \cos(3t), \cos(t) \rangle \cos(t)+ \langle  \cos(3t),\sin(t) \rangle \sin(t)+ \langle  \cos(3t),\cos(2t) \rangle\cos(2t) \\
&+ \langle \cos(3t),\sin(2t) \rangle\sin(2t)  = \frac{\cos(t)}{\pi} \int_{-\pi}^{\pi} \cos(t) \cos(3t) dt  + \frac{\sin(t)}{\pi} \int_{-\pi}^{\pi} \sin(t) \cos(3t) dt + \\
& \frac{\cos(2t)}{\pi} \int_{-\pi}^{\pi} \cos(2t) \cos(3t) dt + \frac{\sin(2t)}{\pi} \int_{-\pi}^{\pi} \sin(2t) \cos(3t) dt  = 0 + 0 + 0 + 0 = 0
\end{aligned}
\end{equation*}
\item [(iv)] 
\begin{equation*}
\begin{aligned}
&\textit{proj}_X(t) = \frac{\cos(t)}{\pi} \int_{-\pi}^{\pi} t\cos(t) dt  + \frac{\sin(t)}{\pi} \int_{-\pi}^{\pi} t\sin(t)dt + \frac{\cos(2t)}{\pi} \int_{-\pi}^{\pi} t\cos(2t)dt \\
& + \frac{\sin(2t)}{\pi} \int_{-\pi}^{\pi} t\sin(2t) dt = \frac{1}{\pi}(0 + 2\pi\sin(t) +0 - \pi\sin(2t)) = 2\sin(t) -\sin(2t)
\end{aligned}
\end{equation*}
\end{itemize}
\end{problem}

\begin{problem}{9.} Let $x = (x_1, x_2)$ and let $y = (y_1, y_2)$. Then $R_\theta x = (\cos(\theta)x_1-\sin(\theta)x_2, \sin(\theta)x_1 + \cos(\theta)x_2)$ and $R_\theta y = (\cos(\theta)y_1-\sin(\theta)y_2, \sin(\theta)y_1 + \cos(\theta)y_2)$. We now expand: 
\begin{equation*}
\begin{aligned}
\langle R_\theta x, R_\theta y \rangle = & (\cos(\theta)x_1-\sin(\theta)x_2)*(\cos(\theta)y_1-\sin(\theta)y_2) + \\& (\sin(\theta)x_1 + \cos(\theta)x_2)*(\sin(\theta)y_1 + \cos(\theta)y_2)
= \\ 
& \cos(\theta)^2x_1y_1 - \sin(\theta)\cos(\theta)x_2y_1 - \cos(\theta)\sin(\theta)x_1y_2 + \sin(\theta)^2x_2y_2 + \\
&  \sin(\theta)^2x_1y_1 + \cos(\theta)^2x_2 y_2 + \cos(\theta)\sin(\theta)x_1y_2 + \cos(\theta)\sin(\theta)x_2y_1 = \\
& (x_1y_1 + x_2y_2)( \cos(\theta)^2 + \sin(\theta)^2)  =  x_1y_1 + x_2y_2 = \langle x, y \rangle
\end{aligned}
\end{equation*}
\end{problem}

\begin{problem}{10.} Let $Q \in M_n(\mathbb{F})$ be an orthonormal matrix. 
\begin{itemize}
\item [(i)] $(\Rightarrow)$ Let $Q \in M_n(\mathbb{F})$ be an orthonormal matrix. Then $\langle Qx, Qy \rangle  = \langle x,y \rangle$ which expanding gives $\langle Qx, Qy \rangle = x^HQ^HQy = x^Hy$. This implies that $Q^HQ = I$. By Proposition 3.2.12, since $Q$ is an orthonormal operator and $\mathbb{F}^n$ is finite dimensional, $Q$ is invertible. Since inverses are unique,  $Q^{-1}  = Q^H$ so $Q^HQ = QQ^H = I$. \\
$(\Leftarrow)$ Let $Q^HQ = QQ^H = I$. Then $\langle Qx, Qy \rangle = x^HQ^HQy = x^HIy = x^Hy = \langle x,y \rangle$.

\item [(ii)] This follows directly from part $i$. $\|Qx\| = \sqrt{\langle Qx, Qx \rangle} = \sqrt{\langle x, x \rangle} = \|x\|$. 
\item [(iii)] By part $i$ we have $Q^{-1}  = Q^H$. Then $\langle Q^Hx, Q^Hy \rangle = x^HQQ^Hy = x^HIy = x^Hy = \langle x,y \rangle$.
\item [(iv)] Consider $\langle Qe_i, Qe_j \rangle = \langle e_i,e_j \rangle = \delta_{ij}$ which is the Kronecker delta. $Qe_i$ is column $i$ of matrix $Q$, so the dot product of column $i$ with itself is 1 and 0 when $i \neq j$, implying the columns of an orthonormal matrix are orthonormal.  
\item [(v)] $\det(QQ^H) = \det(I) = 1$. Since $\det(Q) = \det(Q^H)$ we have $\det(Q)^2= 1$ so $\det(Q)= 1$. The converse is not necessarily true. Consider: 
\[ A= 
\begin{bmatrix}
    1   & 1 & 0 \\
    0   & 1& 0 \\
    0   & 0 &1 
\end{bmatrix}
\]
The matrix $A$ is upper triangular and therefore has $\det(A) = 1$ but is not orthonormal because column 2 dot product itself is not equal to 1 which needs to be true by part $(iv)$. 
\item [(vi)] Let $Q_1, Q_2$ be orthonormal. Then $\langle Q_1Q_2x, Q_1Q_2y \rangle = x^HQ_2^HQ_1^HQ_1Q_2y = x^HQ_2^HIQ_2y = x^HQ_2^HQ_2y = x^Hy = \langle x ,y \rangle$ so the product $Q_1Q_2$ is orthonormal. 
\end{itemize}
\end{problem}

\begin{problem}{11.} Applying the Gram-Schmidt orthonormalization process to a collection of linearly dependent vectors we are ultimately forced to divide by zero when trying to form the orthonormal vector for the first dependent vector in the set. We can see this by setting WLOG the first dependent vector to be the second vector in the set. Then 
$$q_1 = \frac{x_1}{\|x_1\|}$$
$$q_2 = \frac{x_2 - p_2}{\|x_2 - p_2\|}$$
where $p_2 = \langle x_2, q_1 \rangle q_1$. However, assuming $x_2 = a_1 x_1$, we have that 
$$p_2 =  \langle a_1 x_1,  \frac{x_1}{\|x_1\|} \rangle  \frac{x_1}{\|x_1\|} = \langle   \frac{x_1}{\|x_1\|},  \frac{x_1}{\|x_1\|} \rangle   a_1x_1 = a_1x_1 = x_2$$
so $x_2 - p_2 = 0$. 
\end{problem}

\begin{problem}{16.} \hfill
\begin{itemize}
\item [(i)] The $QR$ factorization is not unique. Consider $QR = QIR = QDD^{-1}R$ where $D$ is a diagonal matrix with $-1$ on the diagonal. Because $D$ is diagonal, $D^{-1}$ is also diagonal. Diagonal matrices are upper triangular, and upper triangular matrices are closed under multiplication, so $R' = D^{-1}R$ is upper triangular. Additionally, $Q'  = QD$ is also orthonormal because it is equivalent to $Q$ multiplied by $-1$. Therefore $Q'R' = QR$ and the QR decomposition is not unique.
 
\item [(ii)] Let A be invertible and let there be a QR decompositions $A = Q_1R_1 = Q_2R_2$.  Then $Q_2^HQ^1 = R_2R_1^{-1}$. Both orthonormal and upper triangular matrices are closed under multiplication so the matrix $Q_2^HQ^1$ is both upper triangular and orthonormal. And upper triangular matrix that is orthonormal is a diagonal matrix with either $1$ or $-1$ on the diagonal. Both $R_2$  and  $R_1$ have positive diagonal elements so the matrix $Q_2^HQ^1$ must positive $1$ on the diagonal.  Therefore $I = Q_2^HQ^1 = R_2R_1^{-1}$ so $R_2 = R_1$. 
\end{itemize} 
\end{problem}

\begin{problem}{17.} 
Let $A = \widehat{Q}\widehat{R}$ be reduced QR decomposition. Then the system $A^HAx = A^Hb$ can be rewritten as 
\begin{equation*}
\begin{aligned}
&(\widehat{Q}\widehat{R})^H(\widehat{Q}\widehat{R})x = (\widehat{Q}\widehat{R})^Hb\\
& \Leftrightarrow \widehat{R}^H\widehat{Q}^H\widehat{Q}\widehat{R}x = \widehat{R}^H\widehat{Q}^Hb \\ &\Leftrightarrow \widehat{R}^H\widehat{R}x = \widehat{R}^H\widehat{Q}^Hb \\
&\Leftrightarrow \widehat{R}x = \widehat{Q}^Hb 
\end{aligned}
\end{equation*}
\end{problem}

\begin{problem}{23.} 
We have that $\|x\| = \|x - y + y\| \leq \|x-y\| + \|y\|$ so  $\|x\| - \|y\|  \leq \|x-y\| $. Similarly, $\|y\| - \|x\|  \leq \|x-y\|$ since $\|x-y\| = \|y-x\|$. Putting these together implies that $|\|x\| - \|y\|| \leq \|x-y\|$ because if $\|x\| > \|y\|$ then $|\|x\| - \|y\|| =  \|x\| - \|y\| $ and else $|\|x\| - \|y\|| = \|y\| - \|x\|$.
\end{problem}

\begin{problem}{24.} For each norm we show three things: positivity, scale preservation, and the triangle inequality. Note that the absolute value $| f(t) |$ is nonnegative and only equal to $0$ if the function $f(t) = 0$.
\begin{itemize}
\item [(1)]\begin{itemize}
\item [(i)] By the note above, the integrand $| f(t)|$ is nonnegative. Therefore the integral is non negative. The integral of a non-negative function is only equal to $0$ on $[a,b]$ if the function is equal to $0$ on $[a,b]$. 
\item [(ii)] $\|af\|_{L_1} = \int_a^b |af(t)| dt = \int_a^b |a||f(t)| dt = |a|\int_a^b |f(t)| dt = |a| \|f\|_{L_1}$
\item [(iii)] $\|f + g\|_{L_1} = \int_a^b |f(t) + g(t)| dt \leq \int_a^b( |f(t)| + |g(t)|) dt = \|f\|_{L_1} + \|g\|_{L_1}$

\end{itemize} 
\item [(2)] \begin{itemize}
\item [(i)] By the note above, the integrand $| f(t)|^2$ is nonnegative. Therefore the integral is non negative. The integral of a non-negative function is only equal to $0$ on $[a,b]$ if the function is equal to $0$ on $[a,b]$. Furthermore, the square root is also non-negative. 
\item [(ii)] $\|af\|_{L_2} = \left(\int_a^b |af(t)|^2 dt\right)^{\frac{1}{2}} =  \left(\int_a^b a^2|f(t)|^2 dt\right)^{\frac{1}{2}}  =  |a|\left(\int_a^b |f(t)|^2 dt\right)^{\frac{1}{2}}  = |a| \|f\|_{L_2}$
\item [(iii)] $\|f + g\|_{L_2}^2 = \int_a^b |f(t) + g(t)|^2 dt  \leq \int_a^b (|f(t)| + |g(t)|)^2  = $ \\\\
$ = \int_a^b( |f(t)|^2 + |g(t)|^2 +2|f(t)||g(t)|) dt = (\|f\|_{L_2} + \|g\|_{L_2})^2$\\\\
Therefore $\|f + g\|_{L_2} \leq \|f\|_{L_2} + \|g\|_{L_2}$. 
\end{itemize} 

\item [(3)]
\begin{itemize}
\item [(i)] By the note above, $| f(t)|$ is nonnegative. Then $\sup_{x\in [a,b]} |f(x)| = 0$ if and only if  $| f(x) | = 0$ because if $| f(x) | > 0$ for some $x \in [a,b]$ then by definition $\sup_{x\in [a,b]} |f(x)| > 0$. 
\item [(ii)] $\|af\|_{\infty}=  \sup_{x\in [a,b]} |af(x)| dt =  \sup_{x\in [a,b]}  |a||f(x)|= |a| \sup_{x\in [a,b]} |f(x)|  = |a| \|f\|_{\infty}$
\item [(iii)] $\|f + g\|_{\infty}=  \sup_{x\in [a,b]} |f(x) + g(x)| dt \leq  \sup_{x\in [a,b]} |f(x)| + |g(x)| = \|f\|_{\infty} + \|g\|_{\infty}$
\end{itemize} 
\end{itemize} 
\end{problem}

\begin{problem}{26.} To show that topological equivalence is an equivalence relation we show three things: reflexivity, symmetry, and transitivity. Let $0 < m \leq M$. 
\begin{itemize}
\item [(i)] Let $m  = \frac{1}{2}$ and $M = 2$. Then $m\|\cdot\|_a \leq \|\cdot\|_a \leq M\|\cdot\|_a$ so $\|\cdot\|_a \sim \|\cdot\|_a$. 
\item [(ii)] Let $\|\cdot\|_a \sim \|\cdot\|_b$ so $m\|\cdot\|_a \leq \|\cdot\|_b \leq M\|\cdot\|_a$. Then it follows form the previous inequality that  $\frac{1}{M}\|\cdot\|_b \leq \|\cdot\|_a \leq \frac{1}{m}\|\cdot\|_b$ so $\|\cdot\|_a \sim \|\cdot\|_b$ if and only if $\|\cdot\|_b \sim \|\cdot\|_a$. 
\item [(iii)] Let $\|\cdot\|_a \sim \|\cdot\|_b$ and Let $\|\cdot\|_b \sim \|\cdot\|_c$. Then $m_1\|\cdot\|_a \leq \|\cdot\|_b \leq M_1\|\cdot\|_a$ and $m_2\|\cdot\|_b \leq \|\cdot\|_c \leq M_2\|\cdot\|_b$. It follows then that $m_1m_2\|\cdot\|_a \leq m_2\|\cdot\|_b \leq M_2\|\cdot\|_b \leq M_1M_2\|\cdot\|_a$ and substituting in the second inequality we have that $m_1m_2\|\cdot\|_a \leq m_2\|\cdot\|_b \leq \|\cdot\|_c \leq M_2\|\cdot\|_b \leq M_1M_2\|\cdot\|_a$ so $\|\cdot\|_a \sim \|\cdot\|_c$. 
\end{itemize} 
We now show that the p-norms for $p = 1,2,\infty$ on $\F^n$ are topologically equivalent. 
\begin{itemize}
\item [(i)]  The first inequality follows from $\|x\|_2 = \sqrt{\sum_{i=1}^n |x_i|^2} \leq   \sqrt{\sum_{i=1}^n |x_i|^2  + 2 \sum_{i<j}|x_i||x_j|} =\sum_i^n \sqrt{x_i^2} =  \sum_i^n |x_i| =  \|x\|_1$. For the second inequality consider a vector of $1$'s. Then by Cauchy-Schwarz: $\|x\|_1 = \sum_i^n |1x_i| \leq \sqrt{ \sum_i^n |1| \sum_i^n |x_i|} = \sqrt{n}\sqrt{ \sum_i^n |x_i|} = \sqrt{n}\|x\|_2$ so $\|x\|_1 \sim \|x\|_{2}$
\item [(ii)] Let $\max_i |x_i| =  |x_j|$. Then $\|x\|_{\infty} = \max_i |x_i| =  |x_j| \leq \sqrt{\sum_{i=1}^n |x_i|^2} = \|x\|_2 \leq \sqrt{n |x_j|^2} = \sqrt{n} \|x\|_{\infty}$ so $\|x\|_2 \sim \|x\|_{\infty}$
\end{itemize} 
\end{problem}

 
\begin{problem}{28.} Applying the properties proven in problem 26 we have that:
\begin{itemize}
\item [(i)]   $$\frac{1}{\sqrt{n}}\|A\|_2 =  \sup_{x\neq0}\frac{1}{\sqrt{n}} \frac{\|Ax\|_2}{\|x\|_2} \leq  \sup_{x\neq0} \frac{\|Ax\|_2}{\|x\|_1} \leq  \sup_{x\neq0} \frac{\|Ax\|_1}{\|x\|_1} = \|A\|_1  $$
$$ \leq  \sup_{x\neq0} \sqrt{n}\frac{\|Ax\|_2}{\|x\|_2}= \sqrt{n}\|A\|_2 $$
\item [(ii)] $$ \frac{1}{\sqrt{n}}\|A\|_\infty = \sup_{x\neq0} \frac{1}{\sqrt{n}} \frac{\|Ax\|_\infty}{\|x\|_\infty} \leq  \sup_{x\neq0}  \frac{\|Ax\|_{\infty}}{\|x\|_2} \leq \sup_{x\neq0}  \frac{\|Ax\|_2}{\|x\|_2} = \|A\|_2 $$ $$ \leq  \sup_{x\neq0} \sqrt{n}\frac{\|Ax\|_\infty}{\|x\|_\infty} = \sqrt{n}\|A\|_\infty$$
\end{itemize} 
\end{problem}


\begin{problem}{29.} \hfill
\begin{itemize}
\item[(i)] We proved in problem 10 that $\|Qx\|_2 = \|x\|_2$ if $Q$ is orthonormal. Then $\|Q\|_2 = \sup_{x\neq 0} \frac{\|Qx\|_2}{\|x\|_2} = 1$.  
\item[(ii)] The induced norm of the transformation $R_x$ is $\|R_x\|_2 = \sup_{A\neq 0} \frac{\|Ax\|_2}{\|A\|_2} \leq \sup_{A\neq 0} \frac{\|A\|_2 \|x\|_2}{\|A\|_2} = \|x\|_2$. Now let $A = I$. Then  $\|R_x\|_2 = \sup_{A\neq 0} \frac{\|Ax\|_2}{\|A\|_2} \geq \frac{\|Ix\|_2}{\|I\|} = \|x\|_2$ and hence $\|Rx\|_2 = \|x\|_2$. 
\end{itemize}
\end{problem}


\begin{problem}{30.} To show that $\| \cdot \|_S$ is a matrix norm we show four things:
\begin{itemize}
\item [(i)]  Positivity: $\|A\|_S = \|S(A)S^{-1}\|$ where $\| \cdot \|$ is norm, which being a norm is nonnegative. 
\item [(ii)] Scale: $\|\alpha A\|_S = \|S(\alpha A)S^{-1}\| = |\alpha| S(A)S^{-1}\|= |\alpha| \| A\|_S$
\item [(iii)] Triangle Inequality: $\|A+B\|_S =  \|S(A + B)S^{-1}\| = \|SAS^{-1} + SBS^{-1}\| \leq  \|SAS^{-1}\| + \|SBS^{-1}\| = \|A\|_S + \|B\|_S$. 
\item [(iv)] Submultiplicativity: $\|AB\|_S = \|S(AB)S^{-1}\| = \|SAIBS^{-1}\|  = \|SAS^{-1}SBS^{-1}\| \leq \|SAS^{-1}\|\|SBS^{-1}\| = \|A\|_S\|B\|_S$
\end{itemize} 
\end{problem}

\begin{problem}{37.} 
Let $q=180x^2 -168x+24$. Then
\begin{equation*}
\begin{aligned}
 \langle q,p \rangle &= \int_0^1 qpdx = \int_0^1 (180ax^4 - 168ax^3 + 24ax^2 + 180bx^3 -168bx^2   + 24bx +\\
 & +  180cx^2 - 168cx + 24c)dx = (\frac{180}{5} - \frac{168}{4} + \frac{24}{3})a + (\frac{180}{4} - \frac{168}{3} + \frac{24}{2})b \\
 &+ (\frac{180}{3} - \frac{168}{2} +24)c =  2a + b = p'(1) = L[p]
 \end{aligned}
 \end{equation*}
 We find polynomial q by replacing $180, -168, 24$ with $r,s,t$ respectively and solving backwards from $\langle q,p \rangle = 2a + b$. 
\end{problem}

\begin{problem}{38.} 
Let $D: V \to V$ be the derivative operator where $V  = \F[x;2]$ is a subspace of the inner product space $L^2([0,1]; \R)$. Given the basis $[1,x,x^2]$ we can write the matrix representation of $D$:
\[
   D=
  \left[ {\begin{array}{ccc}
   0 & 1 & 0\\
   0 & 0 & 2\\
   0 & 0 & 0\\
  \end{array} } \right]
\]
Furthermore, the adjoint is the Hermitian, so we can write the matrix representation of $D^*$:
\[
   D^* =
  \left[ {\begin{array}{ccc}
   0 & 0 & 0\\
   1 & 0 & 0\\
   0 & 2 & 0\\
  \end{array} } \right]
\] 

\end{problem}

\begin{problem}{39.} \hfill
\begin{itemize}
\item [(i)]  \begin{itemize}
	\item [(1)]  $\langle v, (S+T)^* w \rangle  = \langle (S+T)v, w \rangle = \langle Sv, w \rangle + \langle Tv, w \rangle = \langle v,S^*w \rangle + \langle v,T^*w \rangle = \langle v, (S^* + T^*)w\rangle$
	\item [(2)] $\langle v, (\alpha T) w \rangle = \alpha\langle (T^*)v, w \rangle  = \langle \bar{ \alpha}(T^*)v, w \rangle $
\end{itemize} 
\item [(ii)] $\langle v, S^*w \rangle  =\langle Sv, w \rangle $  so $S^{*^*} = S$.
\item [(iii)]   $\langle v, (ST)^* w \rangle  = \langle (ST)v, w \rangle =\langle (T)v, (S^*) w \rangle = \langle v, (T^*S^*) w \rangle$
\item [(iv)] First, $\langle T^*(T^{-1})^*v, w \rangle = \langle v, T^{-1}Tw \rangle  = \langle v , w \rangle$. Second, $\langle (T^{-1})^*T^*v, w \rangle = \langle v, TT^{-1}w \rangle  = \langle v , w \rangle$. Therefore, $T^*(T^{-1})^* =(T^{-1})^*T^* =  I$ so $(T^*)^{-1} = (T^{-1})^*$. 
\end{itemize} 
\end{problem}

\begin{problem}{40.} \hfill
\begin{itemize}
\item [(i)] $ \langle A^*B,C \rangle  = \langle B,AC \rangle = tr(B^HAC) = tr((A^HB)^HC) = \langle A^HB,C \rangle$ so $A^* = A^H$. 
\item [(ii)] $\langle A_2, A_3A_1 \rangle = tr(A_2^HA_3A_1) = tr(A_1A_2^HA_3) = tr((A_2A_1^H)^HA_3) = \langle A_2 A_1^H, A_3\rangle =$ \\\\
$=  \langle A_2 A_1^*, A_3\rangle$ where the last equality follows by part (i).
\item [(iii)] $\langle B, T_A(C) \rangle = \langle B, AC -CA \rangle = tr(B^HAC -B^HCA) = tr(B^HAC - AB^HC) = $\\\\
$  = tr((A^HB - BA^H)^HC) = \langle A^HB - BA^H, C \rangle = \langle T_{A^*}(B), C \rangle $

\end{itemize} 
\end{problem}

\begin{problem}{44.} 
Let $A \in M_{m \times n}$, $b \in \F^m$. If the equation $Ax = b$ has a solution then $b \in R(A)$ and it is orthogonal to $N(A^T)$ so if $y \in N(A^T)$ then $\langle y ,b \rangle = 0$. If there is no solution to the equation, then $b \not\in R(A)$ and it is not orthogonal to $N(A^T)$. If $p$ is the projection of $b$ onto $N(A^H)$ then $p^Hb = p^Hp \neq 0$. If $y = \frac{p}{p^Hp}$ then $A^Hy = \frac{A^Hp}{p^Hp} = 0$. Furthermore, $\langle y ,b \rangle = y^Hb = \frac{p^Hb}{p^Hp} = 1 \neq 0$. 
\end{problem}

\begin{problem}{45.} 
We define $Sym_n(\R) = \{A \in M_n(\R) |A^T = A\}$ and $Skew_n(\R) = \{A \in M_n(\R) |A^T = -A\}$. Let $A \in Sym_n(\R)$ and $B \in Skew_n(\R)$. Then $\langle A, B \rangle = tr(A^TB) = tr(BA^T) = - tr(B^TA) =  - \langle B, A \rangle =  - \langle A, B \rangle $ implying that $\langle A, B \rangle = 0$. Furthermore, any matrix $A \in M_N(\R)$ can be written as the sum of a Skew and Sym matrix where $A = \frac{1}{2}(A + A^T) +  \frac{1}{2}(A - A^T)$ which shows that $Sym_n(\R)$ and $Skew_n(\R)$ are orthogonal complements and $Sym_n(\R)^\perp=Skew_n(\R)$.
\end{problem}

\begin{problem}{46.} \hfill
\begin{itemize}
\item [(i)]  Let $x \in N(A^HA)$. Then it is obvious that $A^H (Ax) = A^HAx = 0$ so $Ax \in N(A^H)$. By definition, $Ax \in R(A)$. 
\item [(ii)] Let $x\in N(A)$. Then it follows that $A^H(Ax) = A^H0= 0$ so $x \in N(A^HA)$. Now let $x \in N(A^HA)$. Then by part (i) we know that $Ax \in R(A), N(A^H)$. By the fundamental subspaces theorem,$N(A^H) = R(A)^\perp$ which is orthogonal to $R(A)$. Since $Ax \in R(A) \text{ and } R(A)^\perp$ we have that $Ax = 0$ so $x \in N(A)$. Therefore $N(A) = N(A^HA)$.     
\item [(iii)] According to the rank nullity theorem, if A is a m-by-n matrix, then $rank(A) +  \dim(N(A)) = n$. Rearranging the equation gives $rank(A) = n -  \dim(N(A)) =n -  \dim(N(A^HA)) =  rank(A^HA)$.
\item [(iv)] Let A have linearly independent columns. Then $rank(A) = n = rank(A^HA)$ and by part (iii). Since $A^HA$ is a square n-by-n matrix of rank $n$, it is nonsingular. 
\end{itemize} 
\end{problem}

\begin{problem}{47.} Let $P = A(A^HA)^{-1}A^H$.
\begin{itemize}
\item [(i)]  Since $(A^HA)^{-1}A^HA= I$ we have that $$P^2 =PP= A(A^HA)^{-1}A^HA(A^HA)^{-1}A^H = A(A^HA)^{-1}A^H = P$$
\item [(ii)] $P^H = (A(A^HA)^{-1}A^H)^H = (A^H)^H ((A^HA)^{-1})^H A^H = A ((A^HA)^H)^{-1}A^H = A(A^HA)^{-1}A^H = P$
\item [(iii)] The rank of an idempotent matrix is equal to the trace of the matrix. Therefore, 
$$ rank(P) = tr(P) = tr(A(A^HA)^{-1}A^H) = tr((A^HA)^{-1}A^HA) =  tr(I_{n \times n}) = n$$
\end{itemize} 
\end{problem}

\begin{problem}{48.} \hfill
\begin{itemize}
\item [(i)]  To show linearity we show two things:
\begin{itemize}
\item [(1)]  $P(A + B) = \frac{A + B + (A+B) ^T}{2} = \frac{A + B + A^T +B ^T}{2} = \frac{A +A ^T}{2} + \frac{B +B ^T}{2} = P(A) + P(B)$
\item [(2)]  $P(\alpha A) = \frac{\alpha A +\alpha A ^T}{2} = \alpha \frac{A +A ^T}{2} = \alpha P(A)$
\end{itemize} 
\item [(ii)] $P^2 = P(P(A)) = P\left(\frac{A +A ^T}{2}\right) = \frac{\frac{A +A ^T}{2}+\left(\frac{A +A ^T}{2}\right)^T}{2} = \frac{A + A^T + A^T (A^T)^T}{4} = \frac{2A + 2A^T}{4} = \frac{A + A^T}{2} = P$
\item [(iii)] $\langle P(A), B \rangle = \frac{1}{2}tr(AB + A^TB) = \frac{1}{2}tr(A^TB + A^TB^T) =\langle A, P(B) \rangle$
\item [(iv)]  Let $A \in Skew_n(\R) = \{A \in M_n(\R) |A^T = -A\}$. Then $P(A) = \frac{1}{2}(A +A^T) =  \frac{1}{2}(A -A) = 0$ so $A \in N(P)$. 
\item [(v)]  Let $A \in  Sym_n(\R) = \{A \in M_n(\R) |A^T = A\}$. Then $P(A) = \frac{1}{2}(A +A^T) = \frac{1}{2}(A +A) = A$ so $A \in R(P)$. 
\item [(vi)] $\|A - P(A)\|_F = \sqrt{\langle \frac{1}{2}(A - A^T),\frac{1}{2}(A - A^T) \rangle } = \sqrt{\frac{1}{4}tr((A^T -A)(A-A^T) } =$\\
$=  \sqrt{\frac{1}{4}tr(A^TA -A^2 +AA^T-A^TA^T) } =  \sqrt{\frac{1}{4}tr(2A^TA -A^2 +AA^T-A^2) } = $\\
$ = \sqrt{\frac{1}{2}(tr(A^TA) - tr(A^2))}$
\end{itemize} 
\end{problem}

\begin{problem}{50.} 
We can rewrite $sy^2 +rx^2 = 1$ as $b = Ax$ where 

        \[
          A=
          \begin{bmatrix}
            x_1^2 & y_1^2 \\
            x_2^2 & y_2^2 \\
            \vdots & \vdots \\
            x_n^2 & y_n^2
          \end{bmatrix}
          x =
          \begin{bmatrix}
            r \\
            s
          \end{bmatrix}
          b =  \begin{bmatrix}
            1\\
            \vdots\\
            1
          \end{bmatrix}
           \]
 Then the normal equations are $A^TAx = A^Tb$ where:
 \[
          \begin{bmatrix}
            x_1^2 &  x_2^2  & \cdots & x_n^2 \\
            y_1^2 &  y_2^2  & \cdots & y_n^2
          \end{bmatrix} 
           \begin{bmatrix}
            x_1^2 & y_1^2 \\
            x_2^2 & y_2^2 \\
            \vdots & \vdots \\
            x_n^2 & y_n^2
          \end{bmatrix} = 
         \begin{bmatrix}
            \sum x_i^4 & \sum x_i^2y^2_i\\
            \sum x_i^2y^2_i &  \sum y_i^4 \\
             \end{bmatrix} 
          \]
           \[
          \begin{bmatrix}
            x_1^2 &  x_2^2  & \cdots & x_n^2 \\
            y_1^2 &  y_2^2  & \cdots & y_n^2
          \end{bmatrix} 
          \begin{bmatrix}
            1\\
            \vdots\\
            1
          \end{bmatrix} = 
         \begin{bmatrix}
            \sum x_i^2\\
           \sum y_i^2 \\
             \end{bmatrix} 
          \]

 \end{problem}

\end{document}




